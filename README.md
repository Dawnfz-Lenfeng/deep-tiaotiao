# 跳一跳 AI - DDPG算法改进

## 核心思路

我们采用DDPG（Deep Deterministic Policy Gradient）算法来训练跳一跳AI，但针对游戏特点做了多项改进。

## 算法流程

### 1. 环境初始化
- 定位跳一跳游戏窗口
- 点击重启按钮
- 点击开始按钮
- 获取初始状态（棋子上方区域的28x28灰度图）

### 2. 训练循环
#### 每个Episode:
1. 状态获取
   - 通过模板匹配定位棋子位置
   - 截取棋子上方3倍高度的区域
   - 转换为28x28灰度图作为状态

2. 动作选择
   - epsilon概率随机选择[-1,1]范围的动作
   - 1-epsilon概率使用Actor网络预测动作
   - 将动作值映射为按压时间[300ms,1100ms]

3. 动作执行
   - 在屏幕中心执行按压
   - 等待跳跃动画完成
   - 获取下一个状态

4. 经验存储
   - 将(状态,动作,奖励,下一状态,是否结束)存入优先级回放缓冲区
   - 新经验获得最高优先级

5. 网络更新
   - 当经验足够时，从优先级回放缓冲区采样batch_size个经验
   - 在动作空间采样500个均匀分布的点
   - 计算每个采样点的target_Q值
   - 选择最大的target_Q作为优化目标
   - 更新Critic网络最小化TD误差
   - 更新Actor网络最大化Q值
   - 软更新目标网络
   - 更新经验优先级

6. 探索率更新
   - epsilon = max(epsilon_min, epsilon * epsilon_decay)

### 3. 评估阶段
- 关闭探索（epsilon=0）
- 直接使用Actor网络预测动作
- 记录每个episode的得分
- 保存最佳模型


## 关键改进

### 1. 状态表示优化
- **问题**：直接使用整个游戏窗口截图作为状态会包含历史信息（上一个跳过的方块），影响状态识别的准确性
- **解决方案**：
  - 以棋子位置为基准，向上延伸3个棋子高度作为状态区域
  - 这样可以准确捕获当前需要跳跃的目标方块
  - 状态大小统一缩放为28x28的灰度图

### 2. 探索策略改进
- **原因**：跳一跳是即时奖励问题，每个状态相对独立
- **改进**：
  - 放弃DDPG原有的连续动作空间噪声策略
  - 采用epsilon-greedy策略
  - 定期执行随机动作进行探索
  - 更适合跳一跃这种离散决策场景

### 3. 经验回放优化
- **改进**：实现优先级经验回放（Prioritized Experience Replay）
- **好处**：
  - 更多重复学习有价值的经验
  - 提高样本利用效率
  - 加快学习速度

### 4. 网络结构简化
- **设计理念**：考虑到状态特征相对简单，采用轻量级网络结构
- **具体结构**：
  - 3层卷积网络
  - 去除残差连接
  - 移除批归一化层
  - 保持网络简单高效

### 5. Replay机制创新
- **问题**：原始DDPG在计算目标Q值时容易收敛到次优解
- **改进方案**：
  - 不直接使用target网络预测next_action
  - 在动作空间[-1,1]上均匀采样多个动作点
  - 计算每个采样点的target_Q值
  - 选择最大的target_Q作为优化目标
  - 避免了策略网络的局部最优问题

